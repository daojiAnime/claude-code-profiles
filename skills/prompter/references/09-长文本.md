# 长文本提示技巧

## 长文本的挑战

处理长文本内容时面临的主要挑战：

1. **上下文窗口限制** - Token 数量有上限
2. **信息密度** - 关键信息可能被淹没
3. **处理效率** - 长文本消耗更多资源
4. **注意力分散** - 模型可能忽略某些部分

## 核心策略

### 策略 1: 结构化组织

使用清晰的结构帮助 Claude 定位信息。

**❌ 无结构:**
```
这是一份长达 5000 字的产品需求文档...
[大段文字]

请帮我找出安全相关的需求。
```

**✅ 有结构:**
```xml
<document>
  <section id="overview">
    <title>产品概述</title>
    <content>[内容]</content>
  </section>

  <section id="features">
    <title>功能需求</title>
    <content>[内容]</content>
  </section>

  <section id="security">
    <title>安全需求</title>
    <content>[重点关注这部分]</content>
  </section>

  <section id="performance">
    <title>性能需求</title>
    <content>[内容]</content>
  </section>
</document>

<task>
提取并总结 #security 部分的所有需求
</task>
```

### 策略 2: 文档分块

将长文本分成逻辑块，逐块处理。

```xml
<!-- 方式 A: 顺序处理 -->
<chunk_1>
处理文档的第 1-5 页
[内容]
输出: 第一部分的摘要
</chunk_1>

<chunk_2>
处理文档的第 6-10 页
[内容]
结合第一部分摘要，输出: 综合摘要
</chunk_2>

<chunk_3>
处理文档的第 11-15 页
[内容]
结合前面的摘要，输出: 完整摘要
</chunk_3>
```

```xml
<!-- 方式 B: 并行处理 -->
<parallel_processing>
  <chunk_1>章节 1 分析</chunk_1>
  <chunk_2>章节 2 分析</chunk_2>
  <chunk_3>章节 3 分析</chunk_3>
</parallel_processing>

<aggregation>
汇总所有章节的分析结果
</aggregation>
```

### 策略 3: 索引和引用

为长文本创建索引，精准定位。

```xml
<document_index>
第 1-50 行: 项目背景
第 51-120 行: 技术方案
第 121-200 行: 实施计划
第 201-250 行: 风险评估
</document_index>

<document>
[完整文档]
</document>

<task>
重点分析第 121-200 行的实施计划部分，
评估其可行性和时间安排的合理性。
</task>
```

### 策略 4: 摘要优先

先生成摘要，再根据需要深入。

```xml
<step_1>
对以下长文档生成 200 字摘要:
[长文档]

摘要应包含:
- 主要主题
- 关键论点
- 重要结论
</step_1>

<step_2>
基于摘要: [步骤 1 的输出]

用户关心的问题是: "安全措施是否充分?"

请回到原文档中相关部分，
详细分析安全相关内容。
</step_2>
```

## 实用技巧

### 技巧 1: 使用 XML 标签分隔内容

```xml
<document>
  <metadata>
    <title>年度报告</title>
    <author>财务部</author>
    <date>2024-01-01</date>
  </metadata>

  <executive_summary>
    [执行摘要 - 优先处理这部分]
  </executive_summary>

  <detailed_analysis>
    [详细分析 - 如需要再处理]
  </detailed_analysis>

  <appendix>
    [附录 - 可选参考]
  </appendix>
</document>

<instruction>
先分析执行摘要，如果需要更多细节，
再查看详细分析部分。
</instruction>
```

### 技巧 2: 优先级标记

明确哪些部分最重要。

```xml
<document>
  <section priority="high" id="critical_bugs">
    [紧急 Bug 列表]
  </section>

  <section priority="medium" id="feature_requests">
    [功能请求]
  </section>

  <section priority="low" id="minor_improvements">
    [小改进建议]
  </section>
</document>

<task>
优先处理 high priority 部分，
为每个 bug 生成修复计划。
</task>
```

### 技巧 3: 关键信息提取

预先提取关键点。

```xml
<long_document>
[5000 字的会议记录]
</long_document>

<extraction_task>
第一步: 从会议记录中提取:
1. 所有决策事项 (标记为 DECISION)
2. 所有行动项 (标记为 ACTION)
3. 所有未解决问题 (标记为 OPEN)

第二步: 基于提取的关键点，
生成会议纪要。
</extraction_task>
```

### 技巧 4: 分层摘要

多层次的信息压缩。

```xml
<document>[长文档]</document>

<layer_1>
生成 50 字的核心摘要
</layer_1>

<layer_2>
基于核心摘要，扩展为 200 字的执行摘要
包含主要发现和建议
</layer_2>

<layer_3>
基于执行摘要，生成 500 字的详细摘要
包含支持性证据和具体例子
</layer_3>

用户可以根据需要选择阅读哪一层。
```

## 实战示例

### 示例 1: 法律文件分析

```xml
<legal_document>
  <title>软件许可协议</title>
  <length>12000 字</length>

  <content>
    <section id="definitions" pages="1-2">
      [定义条款]
    </section>

    <section id="grant" pages="3-4">
      [授权范围]
    </section>

    <section id="restrictions" pages="5-7">
      [使用限制] ⚠️ 关键部分
    </section>

    <section id="termination" pages="8-9">
      [终止条款] ⚠️ 关键部分
    </section>

    <section id="warranty" pages="10-11">
      [保证条款]
    </section>

    <section id="liability" pages="12">
      [责任限制] ⚠️ 关键部分
    </section>
  </content>
</legal_document>

<analysis_task>
步骤 1: 重点分析标记为 ⚠️ 的三个关键部分:
  - 使用限制
  - 终止条款
  - 责任限制

步骤 2: 对每个关键部分:
  - 总结主要条款
  - 识别风险点
  - 提出注意事项

步骤 3: 生成风险评估报告 (不超过 500 字)
</analysis_task>
```

### 示例 2: 多文档对比

```xml
<documents>
  <doc id="proposal_v1" length="3000 字">
    <date>2024-01-01</date>
    <content>[第一版提案]</content>
  </doc>

  <doc id="proposal_v2" length="3200 字">
    <date>2024-01-15</date>
    <content>[第二版提案]</content>
  </doc>

  <doc id="proposal_v3" length="3500 字">
    <date>2024-02-01</date>
    <content>[第三版提案]</content>
  </doc>
</documents>

<comparison_task>
步骤 1: 为每个版本生成 150 字摘要

步骤 2: 对比三个版本的摘要，识别:
  - 新增内容
  - 删除内容
  - 修改部分

步骤 3: 重点分析重大变更:
  - 回到原文档查看具体变更
  - 评估变更的影响

步骤 4: 生成变更报告
</comparison_task>
```

### 示例 3: 代码库分析

```xml
<codebase>
  <file path="src/auth.py" lines="500">
    [认证模块代码]
  </file>

  <file path="src/api.py" lines="800">
    [API 模块代码]
  </file>

  <file path="src/database.py" lines="600">
    [数据库模块代码]
  </file>

  <file path="tests/" lines="1200">
    [测试代码]
  </file>
</codebase>

<review_task>
步骤 1: 为每个文件生成摘要:
  - 主要功能
  - 导出的 API
  - 外部依赖

步骤 2: 分析模块间的依赖关系

步骤 3: 针对用户问题 "如何添加新的认证方式":
  - 定位相关代码 (auth.py)
  - 分析现有认证流程
  - 提出扩展方案

步骤 4: 检查测试覆盖
</review_task>
```

## 高级技巧

### 技巧 1: 窗口滑动

处理超长文本时使用滑动窗口。

```xml
<strategy>
文档长度: 10000 字
窗口大小: 2000 字
重叠: 200 字

窗口 1: 字符 0-2000
窗口 2: 字符 1800-3800 (重叠 200)
窗口 3: 字符 3600-5600 (重叠 200)
...

对每个窗口:
  1. 提取关键信息
  2. 识别是否需要上下文
  3. 合并相邻窗口的发现
</strategy>
```

### 技巧 2: 按需加载

只在需要时加载详细内容。

```xml
<document_structure>
<toc>
1. 引言 (第 1-2 页)
2. 方法论 (第 3-10 页)
3. 实验结果 (第 11-25 页)
4. 讨论 (第 26-30 页)
5. 结论 (第 31-32 页)
</toc>
</document_structure>

<initial_task>
用户问题: "这篇论文的主要发现是什么?"

步骤 1: 阅读引言和结论 (4 页)
步骤 2: 如果需要更多细节，再加载实验结果部分
</initial_task>
```

### 技巧 3: Map-Reduce 模式

```xml
<!-- Map 阶段: 分块处理 -->
<map_phase>
  将文档分成 10 个块，对每个块:
  提取: 主题、关键词、主要观点
</map_phase>

<!-- Reduce 阶段: 汇总 -->
<reduce_phase>
  汇总所有块的提取结果:
  - 合并相似主题
  - 统计关键词频率
  - 综合主要观点
  - 生成整体摘要
</reduce_phase>
```

### 技巧 4: 金字塔原则

先给结论，再提供细节支持。

```xml
<pyramid_structure>
  <level_1_conclusion>
    最重要的发现 (50 字)
  </level_1_conclusion>

  <level_2_key_points>
    支持结论的 3-5 个关键点 (200 字)
  </level_2_key_points>

  <level_3_details>
    每个关键点的详细说明 (500 字)
  </level_3_details>

  <level_4_evidence>
    完整的证据和数据 (引用长文档)
  </level_4_evidence>
</pyramid_structure>

用户可以选择阅读到哪一层深度。
```

## 最佳实践

### ✅ 推荐做法

1. **使用 XML 标签结构化**
   - 清晰的层次
   - 明确的边界
   - 便于引用

2. **提供导航信息**
   - 目录
   - 页码
   - 章节标记

3. **标注优先级**
   - 关键部分高亮
   - 可选部分标注
   - 提供略读指引

4. **分块处理**
   - 逻辑分块
   - 保持上下文
   - 渐进式深入

### ❌ 避免的陷阱

1. **一次处理全部**
   - 容易超出限制
   - 难以聚焦重点

2. **丢失上下文**
   - 分块时丢失连接
   - 摘要过度简化

3. **无结构堆砌**
   - 大段未分类文本
   - 缺少索引和标记

## 工具和技术

### 文本预处理

```python
# 示例: 将长文档分块
def chunk_document(text, chunk_size=2000, overlap=200):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append({
            'id': len(chunks),
            'start': start,
            'end': end,
            'content': chunk
        })
        start = end - overlap
    return chunks
```

### 摘要模板

```xml
<summary_template>
  <document_meta>
    标题: [标题]
    长度: [字数]
    主题: [主题标签]
  </document_meta>

  <tl_dr>
    一句话总结: [核心信息]
  </tl_dr>

  <executive_summary>
    执行摘要 (200 字):
    - 背景
    - 主要发现
    - 关键建议
  </executive_summary>

  <detailed_summary>
    详细摘要 (500 字):
    [按章节展开]
  </detailed_summary>

  <key_takeaways>
    关键要点 (bullet points):
    - 要点 1
    - 要点 2
    - 要点 3
  </key_takeaways>
</summary_template>
```

## 总结

处理长文本的关键策略：

- 📊 结构化组织内容
- 🔍 优先级和索引
- 📦 合理分块处理
- 🎯 摘要优先，按需深入
- 🔗 保持上下文连接
- ⚡ Map-Reduce 并行处理
- 📝 金字塔式呈现
